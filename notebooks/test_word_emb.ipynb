{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "ordered_design_keys = ['基础廓形', '肩', '门襟', '结构', '腰节', '摆', '领型']\n",
    "\n",
    "design_vars = {\n",
    "    \"基础廓形\": ['H型', 'X型', 'A型', 'O型', 'T恤', '紧身', '宽松', '合体'],\n",
    "    \"结构\": ['无省', '收腰省', '袖窿公主线分割', '直线公主线分割', '公主线', '公主缝'],\n",
    "    \"腰节\": ['无腰节', '断腰节', '拼腰节', '抽皱腰节', '单向褶', '风琴折'],\n",
    "    \"摆\": ['无摆', '荷叶摆', '荷叶边', '木耳边', '拼接抽皱', '不规则下摆', '两侧长中间短下摆', '前短后长下摆'],\n",
    "    \"领型\": ['有领', '无领', 'V领', '圆领', '椭圆形领', 'U型领', '马蹄型领', '缺口领', '叠领', '套装领', '打裥领', '荡领', '心型领', '船领', '梯形领', '鸡心领', '高领', '高垂领', '锁孔领', '花瓣型领', '漏斗领', '水滴领', '亨利领', '抽绳式立领', '方领', '不对称领', '钻石领', '船员领', '立领', '衬衫领', '木耳边领', '娃娃领', '海军领', '蕾丝花边领', '切尔西式领', '中式马褂型领', '开领', '离颈领', '小丑领', '燕子领', 'POLO领', '连身立领'],\n",
    "    \"门襟\": ['无门襟', '交叉门襟', '叠门襟', '半开门襟'],\n",
    "    \"肩\": ['正常肩', '正肩', '落肩', '连身袖', '连身插角袖插块', '连身插角袖插片', '插肩袖'],\n",
    "    \"袖\": ['无袖', '小飞袖', '天使款大衣袖', '袖山抽皱泡泡袖', '一片袖', '两片袖', '露肩袖', '藕节袖', '羊腿袖', '漏斗袖', '喇叭袖', '宝塔袖', '淑女袖', '缺角袖', '主教服式袖', '泡泡袖', '灯笼袖', '气球型袖', '蝴蝶袖', '层叠袖', '手帕袖', '衬衫袖', '木耳袖口', '娜塔夫袖口']\n",
    "}\n",
    "\n",
    "####################### 1. Load data items (RAW) #######################\n",
    "# data_roots = [\n",
    "#     \"\\\\\\\\192.168.29.222\\\\Share\\\\工程数据Q1\\\\objs\", \n",
    "#     # \"\\\\\\\\192.168.29.222\\\\Share\\\\工程数据Q2_objs\"\n",
    "#     ]\n",
    "# all_items = []\n",
    "# print(\">>> Locating data items...\")\n",
    "# for data_root in data_roots:\n",
    "#     cur_items = glob(os.path.join(data_root, '**', 'pattern.json'), recursive=True)\n",
    "#     cur_items = [os.path.dirname(x) for x in cur_items]\n",
    "#     all_items += cur_items\n",
    "#     print(\"%d items in %s.\"%(len(cur_items), data_root))\n",
    "# print('[DONE] locating data items, total %d items.'%(len(all_items)))\n",
    "########################################################################\n",
    "\n",
    "####################### 2. Load data items (PICKLE) #######################\n",
    "# data_root = \"E:\\\\lry\\\\data\\\\AIGP\\\\demo_v2\\\\Q2\\\\brep_uni_norm\\\\\"\n",
    "# data_dirs = glob(os.path.join(data_root, '*.pkl'))\n",
    "# all_items = []\n",
    "# for data_item in tqdm(data_dirs):\n",
    "#     with open(data_item, \"rb\") as tf: data = pickle.load(tf)\n",
    "#     data_fp = os.path.basename(data['data_fp'])\n",
    "#     data_fp = re.sub(r'\\d+', '', data_fp).replace('-', '').replace('_', '').strip().upper()\n",
    "#     all_items.append(data_fp)\n",
    "# print('[DONE] Loading data items: ', len(all_items), all_items[:10])\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "####################### 3. Load data items (JSON) #######################\n",
    "data_root = \"E:\\\\lry\\\\data\\\\AIGP\\\\demo_v2\\\\Q2\\\\patterns\\\\\"\n",
    "data_dirs = glob(os.path.join(data_root, '*.json'))\n",
    "all_items = []\n",
    "for data_item in tqdm(data_dirs):\n",
    "    with open(data_item, \"rb\") as tf: data = pickle.load(tf)\n",
    "    data_fp = os.path.basename(data['data_fp'])\n",
    "    data_fp = re.sub(r'\\d+', '', data_fp).replace('-', '').replace('_', '').strip().upper()\n",
    "    all_items.append(data_fp)\n",
    "print('[DONE] Loading data items: ', len(all_items), all_items[:10])\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "print('>>> Try word cutting...')\n",
    "jieba.load_userdict('dict.txt')\n",
    "total_words = []\n",
    "with open('word_cut_result.txt', 'w') as f:\n",
    "    for sentence in tqdm(all_items):\n",
    "        words = jieba.lcut(sentence, cut_all=False)\n",
    "        total_words += words\n",
    "        f.write(\"%s => %s\\n\" % (sentence, '|'.join(words)))\n",
    "print('[DONE] Total words after cut: ', len(total_words))\n",
    "\n",
    "# total_words = [x.upper() for x in total_words]\n",
    "# total_words_freq = dict([(x, total_words.count(x)) for x in total_words if x.strip()])\n",
    "# total_words_freq = sorted(total_words_freq.items(), key=lambda x: x[1], reverse=True)    \n",
    "\n",
    "# design_var_dict = {}\n",
    "# for key in design_vars:\n",
    "#     for val in design_vars[key]:\n",
    "#         design_var_dict[val] = key\n",
    "\n",
    "# totel_words_freq_category = {\"其他\": []}\n",
    "# for word, freq in total_words_freq:\n",
    "#     word = word.strip()\n",
    "#     if not word: continue\n",
    "#     elif word.upper() in design_var_dict:\n",
    "#         if design_var_dict[word] not in totel_words_freq_category: totel_words_freq_category[design_var_dict[word]] = []\n",
    "#         totel_words_freq_category[design_var_dict[word]].append((word, freq))\n",
    "#     else:\n",
    "#         totel_words_freq_category[\"其他\"].append((word, freq))\n",
    "\n",
    "# with open('word_freq.txt', 'w') as f:\n",
    "#     for key in design_vars.keys():\n",
    "#         f.write(key+':\\n')\n",
    "#         for word, freq in totel_words_freq_category[key]:\n",
    "#             if word.strip() == '': continue\n",
    "#             f.write('\\t'+word+' '+str(freq)+'\\n')\n",
    "#         f.write('\\n')\n",
    "#     f.write(\"其他:\\n\")\n",
    "#     for word, freq in totel_words_freq_category[\"其他\"]:\n",
    "#         if word.strip() == '': continue\n",
    "#         if freq < 10: continue\n",
    "#         f.write('\\t'+word+' '+str(freq)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11487/11487 [03:36<00:00, 53.12it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "ordered_design_keys = ['基础廓形', '肩', '门襟', '结构', '腰节', '摆', '领型']\n",
    "\n",
    "design_vars = {\n",
    "    \"基础廓形\": ['H型', 'X型', 'A型', 'O型', 'T恤', '紧身', '宽松', '合体'],\n",
    "    \"结构\": ['无省', '收腰省', '袖窿公主线分割', '直线公主线分割', '公主线', '公主缝'],\n",
    "    \"腰节\": ['无腰节', '断腰节', '拼腰节', '抽皱腰节', '单向褶', '风琴折'],\n",
    "    \"摆\": ['无摆', '荷叶摆', '荷叶边', '木耳边', '拼接抽皱', '不规则下摆', '两侧长中间短下摆', '前短后长下摆'],\n",
    "    \"领型\": ['有领', '无领', 'V领', '圆领', '椭圆形领', 'U型领', '马蹄型领', '缺口领', '叠领', '套装领', '打裥领', '荡领', '心型领', '船领', '梯形领', '鸡心领', '高领', '高垂领', '锁孔领', '花瓣型领', '漏斗领', '水滴领', '亨利领', '抽绳式立领', '方领', '不对称领', '钻石领', '船员领', '立领', '衬衫领', '木耳边领', '娃娃领', '海军领', '蕾丝花边领', '切尔西式领', '中式马褂型领', '开领', '离颈领', '小丑领', '燕子领', 'POLO领', '连身立领'],\n",
    "    \"门襟\": ['无门襟', '交叉门襟', '叠门襟', '半开门襟'],\n",
    "    \"肩\": ['正常肩', '正肩', '落肩', '连身袖', '连身插角袖插块', '连身插角袖插片', '插肩袖'],\n",
    "    \"袖\": ['无袖', '小飞袖', '天使款大衣袖', '袖山抽皱泡泡袖', '一片袖', '两片袖', '露肩袖', '藕节袖', '羊腿袖', '漏斗袖', '喇叭袖', '宝塔袖', '淑女袖', '缺角袖', '主教服式袖', '泡泡袖', '灯笼袖', '气球型袖', '蝴蝶袖', '层叠袖', '手帕袖', '衬衫袖', '木耳袖口', '娜塔夫袖口']\n",
    "}\n",
    "\n",
    "\n",
    "jieba.load_userdict('dict.txt')\n",
    "\n",
    "####################### 3. Load data items (JSON) #######################\n",
    "data_root = \"E:\\\\lry\\\\data\\\\AIGP\\\\demo_v2\\\\Q2\\\\patterns\\\\\"\n",
    "output_root = \"E:\\\\lry\\\\data\\\\AIGP\\\\demo_v2\\\\Q2\\\\patterns_with_caption\\\\\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "data_dirs = glob(os.path.join(data_root, '*.json'))\n",
    "all_items = []\n",
    "for data_item in tqdm(data_dirs):\n",
    "    try:\n",
    "    \n",
    "        with open(data_item, \"r\") as tf: pattern_spec = json.load(tf)\n",
    "        data_caption = os.path.basename(pattern_spec[\"raw_data_fp\"])\n",
    "        data_caption = re.sub(r'\\d+', '', data_caption).replace('-', '').replace('_', '').strip().upper()\n",
    "        data_caption = jieba.lcut(data_caption, cut_all=False)\n",
    "        data_caption = [x for x in data_caption if x.strip() != \"连衣裙\"]\n",
    "        data_caption = [\"连衣裙\", \", \".join(data_caption)]\n",
    "            \n",
    "        pattern_spec[\"caption\"] = data_caption\n",
    "        \n",
    "        with open(os.path.join(output_root, os.path.basename(data_item)), \"w\", encoding='utf-8') as tf:\n",
    "            json.dump(pattern_spec, tf, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('[ERROR] ', data_item, e)\n",
    "    \n",
    "###########################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anno_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
